https://medium.com/@muhammadluay45/sentiment-analysis-using-recurrent-neural-network-rnn-long-short-term-memory-lstm-and-38d6e670173f, https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/, https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense, Embedding
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
import numpy as np
from keras.layers import Bidirectional, LSTM


dataset = None

# look into changing num_words and oov_token
tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<OOV>')
train_sentences, train_labels = zip(*[(sent.numpy().decode('utf8'), label.numpy()) for sent, label in train_data])
tokenizer.fit_on_texts(train_sentences)
train_sequences = tokenizer.texts_to_sequences(train_sentences)

# look into changing maxlen, padding, and truncating
train_padded = keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=100, padding='post', truncating='post')

test_sentences, test_labels = zip(*[(sent.numpy().decode('utf8'), label.numpy()) for sent, label in test_data])
test_sequences = tokenizer.texts_to_sequences(test_sentences)

# look into changing maxlen, padding, and truncating
test_padded = keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=100, padding='post', truncating='post')

train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

# number of subsets that dataset will be divided into 
batch_size = 32

# number of iterations through the dataset
epochs = 10

model = keras.models.Sequential([
    keras.layers.Embedding(10000, 32), #10000 vocab size of text
    #keras.layers.Dropout(0.5), turns off 50% of neurons to avoid overfitting
    keras.layers.SimpleRNN(32, kernel_regularizer=keras.regularizers.l2(0.001)) L2 add penalty to loss function to encourage model to have smaller weights to prevent overfitting
    keras.layers.Dense(1, activation='sigmoid') #output between 0 and 1
])

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2) training is stopped if validation doesn't improved for a specified number of epochs(in this case 3)

# compile the model
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels))

model.summary()

scores = model.evaluate(X_test, y_test, verbose=0)
print('Test accuracy:', scores[1])


