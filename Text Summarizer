# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/, https://plainenglish.io/blog/how-to-generate-word-embedding-using-bert, https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390
import ntlk
import numpy as np
import pandas as pd
nltk.download('punkt')
import re
from nltk.tokenize import sent_tokenize
from transformers import BertTokenizer
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

#cleaning sentences must be done first and foremost

def split_into_tokens(text):
  sentences = nltk.sent_tokenize(text)
  return sentences

#could use FullTokenizer instead

#sentences = sent_tokenize(text)
#tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
#tokenized_sentences = [tokenizer(sentence)["input_ids"] for sentence in sentences]
#input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentences)
#input_ids = tf.expand_dims(input_ids, 0)
#outputs = bert_layer(input_ids)
#embeddings = outputs["sequence_output"][0]

df = "link to financial articles on stocks"

word_embeddings = {}

f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:], dtype='float32')
  word_embeddings[word] = coefs
f.close()

#clean text using the clean text method which will get you cleaned_sentences

sentence_vectors = []
for cleaned_sentence in cleaned_sentences:
  if len(cleaned_sentence) != 0:
    curr_vector = sum([word_embeddings.get(w, np.zeros(100,))) for w in cleaned_sentence.split()])/(len(cleaned_sentence.split()) + 0.001)
  else:
    curr_vector = np.zeros((100,))
  sentence_vectors.append(curr_vector)

similarity_matrix = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
  for j in range(len(sentences)):
    if i != j:
      similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]

#view the graph of the textrank

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

#get the top N sentences based on their ranking
n = 5
for i in range(n):
print(ranked_sentences[i][1])
