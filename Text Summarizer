# https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/, https://plainenglish.io/blog/how-to-generate-word-embedding-using-bert, https://medium.com/data-science-in-your-pocket/text-summarization-using-textrank-in-nlp-4bce52c5b390
import ntlk
import numpy as np
import pandas as pd
nltk.download('punkt')
import re
from nltk.tokenize import sent_tokenize
from transformers import BertTokenizer
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import networkx as nx

#cleaning sentences must be done first and foremost

def split_into_tokens(text):
  sentences = nltk.sent_tokenize(text)
  return sentences

"""
model = SentenceTransformer('bert-base-nli-mean-tokens')  # Example model, choose based on your needs
word_embeddings = model.encode(sentences)
"""

"""
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained('bert-base-uncased')

embeddings = len(sentences)

for cleaned_sentence in sentences:
  # tokenizes the inputted text and return tensors from pytorch
  bert_inputs = tokenizer(cleaned_sentences, return_tensors = 'pt')

  # obtain the embedding
  with torch.no_grad():
    outputs = model(**bert_inputs)
  last_hidden_state = outputs.last_hidden_state
  sentence_embedding = torch.mean(last_hidden_states, dim=1).numpy()

similarities = cosine_similarity(query_embedding, np.vstack(embeddings))
"""

df = "link to financial articles on stocks"

word_embeddings = {}

f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:], dtype='float32')
  word_embeddings[word] = coefs
f.close()

sentence_vectors = []
for cleaned_sentence in cleaned_sentences:
  if len(cleaned_sentence) != 0:
    curr_vector = sum([word_embeddings.get(w, np.zeros(100,))) for w in cleaned_sentence.split()])/(len(cleaned_sentence.split()) + 0.001)
  else:
    curr_vector = np.zeros((100,))
  sentence_vectors.append(curr_vector)

similarity_matrix = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
  for j in range(len(sentences)):
    if i != j:
      similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]

#view the graph of the textrank

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

#get the top N sentences based on their ranking
n = 5
for i in range(n):
print(ranked_sentences[i][1])
